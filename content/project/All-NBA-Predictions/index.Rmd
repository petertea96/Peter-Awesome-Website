---
title: Predicting All-NBA Teams
author: Peter Tea
date: '2020-04-29'
categories:
  - Basketball
tags:
  - Prediction
  - NBA
  - Basketball
authors:
  - admin
output:
  html_document:
    keep_md: yes
image:
  caption: Photo by JC Gellidon on Unsplash
  focal_point: Smart
summary: Predicting the All-NBA teams
---

```{r setup, include=FALSE, eval=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 2020 All-NBA Predictions

Who will be named to the All-NBA teams this year? In this project, we fit several classification models on per 100 possessions data to vote in this year's All-NBA selection. Then, we aggregate the results from each model that mimics the current All-NBA voting system. 

## Table of contents
1. [Introduction: All-NBA Selection](#introduction)
2. [Obtaining/Scraping Data](#data)
3. [Model Votes](#votes)
4. [Logistic Regression](#logreg)
4. [Random Forest](#rfc)
4. [GAM](#gam) 
4. [KNN](#knn)
4. [SVM](#svm) 
5. [Features](#features)

## All-NBA Selection <a name="introduction"></a>

The All-NBA award is an annual NBA distinction bestowed to players with outstanding performances after a given season. Since 1989, the All-NBA award has been consistently partitioned into 3 tiers; 1st, 2nd and 3rd All-NBA teams where each team is uniquely composed of 2 guards, 2 forwards and 1 center. 

<!--
In total, 15 players partioned by 6 guards, 6 forwards and 3 centers are selected. 
Not only is there a financial incentive for players implicated with this award, but there is also a reputation aspect which may in some way validate the chosen players as the true stars in the game. 
-->


The NBA holds a firm reputation of being a ``Star-Driven League``, meaning that teams who boast the top players on their active rosters are the ones who can legitimately and exclusively contend for championships. The identification of All-NBA caliber players provides an interesting avenue where we can explore which players can serve as foundational pieces, and that may eventually land a fortunate team an elusive championship. 

While NBA pundits are accustomed to making predictions based on preconceived biases and opinions on players, we make our own All-NBA predictions using a set of classification models which hopefully reduces the emotional elements and recall biases plaguing NBA voting.


<br/>

## Obtaining/Scraping NBA Data <a name="data"></a>

Box-score metrics standardized by *per 100 possessions* was obtained through R, using the [bballR](https://rdrr.io/github/bobbyingram/bballR/man/scrape_all_players.html) R package.
We chose to look at per 100 possessions instead of the conventional per game standard since per 100 possessions provides a more intuitive comparison between players under different eras and team offence styles. 

Past All-NBA winners dating back from 1989 onward to 2019 was scraped through [Basketball Reference](https://www.basketball-reference.com/) with the ``rVest`` R package.

To further reduce noise in the data, we chose to only consider players meeting the following 2 conditions:

1. Player participated in at least 10 games
2. Player played at least 25 minutes/game

These conditions remove players who are not full-time NBA players and is aimed at choosing a representative sample of NBA players who are more inclined to be considered for the All-NBA teams. 


### Data Inconsistencies

Many issues were faced when collecting data from different sources. One main concern was that player position data were inconsistent. For example in the 2017-18 season Anthony Davis was coded as a Forward, despite winning All-NBA award that year as a Center. Similarly in that same year, both Jimmy Butler and Lamarcus Aldridge were coded as a Guard and Center respectively, despite both being named onto the All-NBA as Forwards. Manual adjustments on player positions were made to adher to the position coded in the All-NBA data but since not all players are in the All-NBA data, it is very likely that other position inconsistencies were left untreated in the data.

<br/>

## Model Votes <a name="votes"></a>

In total, 6 models including Logistic Regression, Random Forest, Generalized Additive Model (GAM), K-Nearest-Neighbours (KNN), Support Vector Machine (SVM) and XGBoost were fitted onto our data. Each model provided a probability estimate for each player being named to a 2020 All-NBA team, which we then ranked from greatest to least by position. We then mimic the current voting rules where for each voter, the top 2 ranked guards, top 2 forwards and top centre are awarded 3 voting points. The subsequent top 2 ranked guards, 2 forwards and center are awarded 2 voting points and the next sequence are awarded 1 voting point. We then add an extra sequence of players, which were awarded 0.5 voting points. 

The prediction results are as follows:

### Predicted 2020 All-NBA 1st Team
| Player | Position | Votes |
|---|---|---|
| Luka Dončić | Guard | 18 |
| James Harden | Guard  | 18  |
| Giannis Antetokounmpo  | Forward  | 18  |
| LeBron James | Forward  |  17 |
| Anthony Davis | Center  | 18 |

### Predicted 2020 All-NBA 2nd Team
| Player | Position | Votes |
|---|---|---|
| Russell Westbrook | Guard |  10 |
| Damian Lillard | Guard  | 9.5  |
| Kawhi Leonard | Forward  | 13  |
| Jimmy Butler | Forward  | 7 |
| Nikola Jokić | Center  | 9  |



### Predicted 2020 All-NBA 3rd Team
| Player | Position | Votes |
|---|---|---|
| Trae Young | Guard | 5.5 |
| Donovan Mitchell | Guard  | 4 |
| Khris Middleton | Forward  | 6.5 |
| Jayson Tatum | Forward  | 5 |
| Rudy Gobert | Center | 8.5 |

### Honourable mentions
Guards: Bradley Beal (3.5 votes), Kemba Walker (3 votes), Kyle Lowry (2 votes)

Forwards: Bam Adebayo (4 votes), Pascal Siakam (3 votes), Paul George (2 votes)

Center: Joel Embiid (3.5 votes)



<br/>


### Model Comparisons

To ensure fair and consistent comparison, we fit each model using the same training set and evaluate performance using the same test set. Since there was a huge imbalance in the number of All-NBA players vs. the number of Non-All-NBA players, we chose to obtain the train-test split with identical proportion of All-NBA cases. Here is the breakdown of our training and test data:

| Data	| # 1st All NBA Team Players | 	# 2nd All NBA Team Players| # 3rd All NBA Team Players| 	# Non-All-NBA Players| 
|-------|------|------|------|------|
|Training Data	|116	|116	|116	|3625|
|Test Data |	39 |	39 |	39 |	1088|
|Total	| 155 |	155 |	155 |	4713 |


Both the training and test sets are composed of roughly 90% Non-All-NBA players, and 3.3% each of 1st, 2nd and 3rd All-NBA players. In total, the training set contained 348 All-NBA player observations (116 x 3 = 348), while the test set contained 117 All-NBA player observations (39 x 3 = 117). 


We evaluate the performances of each model on the test set with the following metrics:

1.	How well the model identifies players who ultimately end up on the 1st, 2nd or 3rd All-NBA teams. We define ith Team Sensitivity as:

```
  ith Team Sensitivity =  # True Positives on ith All-NBA team / Total number players on the ith All-NBA team
  
  where i = 1, 2, 3.
```
2. Overall All-NBA sensitivity
```
  Overall Sensitivity =  # True Positives on any All-NBA team / Total number players on any All-NBA team
```

2.	How well the model properly identifies Non-All-NBA players. We define the False Positive Rate (FPR) as
```
  FPR =  # False Positive All-NBA players / Total number Non-All-NBA players
```
```
   Note: True Positives represent the number of correctly predicted All-NBA players, while False Positives represent the number of incorrectly predicted All-NBA players.
```

Ideally, we want models to have high sensitivity and low FPR.
1st Team All-NBA players should in theory be the easiest for the models to identify, since these players represent the supposed elite top 5 players after a given season. On the other hand, players who ultimately end up on the 2nd and 3rd All-NBA teams might be tougher for the models to identify; usually there is less consensus among voters on players occupying the secondary and tertiary teams.


Below, we present a table comparing our 5 models based on the metrics described above.
<br/>


```{r, performance, echo=FALSE, warning=FALSE}
perf = read.csv("performances_dat.csv")
#"model_performance_comparisons.csv"
colnames(perf) <- c("All-NBA Team", "Logistic Regression", "Random Forest","GAM", "KNN",
                    "SVM", "XGBoost", "True Total")

overall_sens = round(colSums(perf[-(4), -c(1,8)])/ (39*3),3)


perf[1:3,c(2:7)] <- round(perf[1:3,c(2:7)]/perf[1:3,8], 3)
perf[4,c(2:7)] <- round((perf[4,c(2:7)])/1088, 3)

part1 = rbind(perf[-4,  -c(1,8)], overall_sens, perf[4,c(2:7)])

col1 = c("1st All-NBA Sensitivity", "2nd All-NBA Sensitivity", "3rd All-NBA Sensitivity",
         "Overall All-NBA Sensitivity","FPR")

part2 = cbind(col1, part1)

colnames(part2) = c("Performance Metric","Logistic Regression", "Random Forest","GAM", "KNN","SVM", "XGBoost" )
knitr::kable(part2,
             digits = 2)
```
<br/>


How do we interpret these results? Let’s consider the logistic regression performance row. We see that this model was able to correctly identify 92.3 % of 1st team All-NBA players in the test set. That’s 36 out of 39 1st team players identified, which also means that this model was unable to identify three 1st All-NBA players (fyi these 3 players were: Guard Latrell Sprewell in 1994, Guard Jason Kidd in 1999 and Center Marc Gasol in 2015).  Furthermore, the model was able to correctly identify 71.8 % of 2nd team All-NBA players (28 of 39) and 51.3% of 3rd team All-NBA players (20 of 39). 
<!--
The fitted logistic regression model had a test dataset precision of 79% and 97% for players making All-NBA and players not making All-NBA, respectively. This means that of the All-NBA predictions made, the model was correct 79% of the time while of the non-All-NBA predictions made, the model was correct 97% of the time.
-->

In general, we see that all models have an easier time predicting players on the 1st team All-NBA compared to the remaining 2 All-NBA teams. Intuitively this means that the model is easily able to identify the greatest players (ranked 1 – 5) but struggles in identifying the last selections (ranked 11 – 15). We also generally see a tradeoff between a model's sensitivity and their FPR. Take for example GAM which has the highest sensitivities across all 3 All-NBA teams, but also the highest FPR when compared against its competing models. The GAM model makes a higher volume of All-NBA predictions than its competitors, which leads to both a higher volume of True Positives (identifiying true All-NBA players) and False Negatives (misidentifying All-NBA players).




<br/>

## Logistic Regression <a name="logreg"></a>

Logistic regression was the first classification algorithm fitted to our All-NBA data. Players with a predicted probability of making the All-NBA greater than 50% were classified as All-NBA predictions in the traning and test stages.

### Logistic Regression 2020 Predictions

Here, we present the model's All-NBA probability predictions on the 2020 season.

```{r, logregpred, echo=FALSE}
log_pred = read.csv("logreg_2020_predictions.csv")

to_show = log_pred[, -c(2,3,5)]

to_show$Probability = round(to_show$Probability,3)*100

knitr::kable(to_show, 
             caption = "Logistic Regression 2020 All-NBA predictions",
             digits = 2)
```


## Random Forest Classifier <a name="rfc"></a>

A Random forest algorithm was the second classification algorithm fitted to our All-NBA data. The formation of nodes was made based on a gini index criterion (i.e. splitting nodes based on minimizing gini impurity). Other criterion, like entropy, was considered however was found to have comparable performance in terms of precision. The binary classification of All-NBA selection vs. Non-All-NBA selection was made with a threshold of 50% of votes from the aggregate decision trees. Other threshold values were considered, however was found to have lower precision. In total, we set 200 decision trees for the random forest. 


### Random Forest 2020 Predictions

The probability estimates obtained represent the proportion among the 200 decision trees that predict an All-NBA selection.

```{r, rfcpred, echo = FALSE}
rfc_pred = read.csv("rfc_predictions_2020.csv")

to_show = rfc_pred[, -c(2,3,5)]

to_show$Probability = round(to_show$Probability,3)*100


knitr::kable(to_show, 
             caption = "Random Forest 2020 All-NBA predictions",
             digits = 2)

```

### Random Forest Feature Importance
With random forests, we also evaluate the relative importance of each feature in the fitted model. This is done by calculating the relevance score of each feature, standardized such that the sum of all feature scores is 1.

![](feature_importance.png)

From the Feature Importance plot, we see that Points scored is by and large the most important feature in deciding whether a player is predicted to be All-NBA. This is then followed by Field goals made and then the overall performance of the team. It has been frequently suggested, especially by members of the media who participate in All-NBA voting, that players who put up great numbers on winning teams are viewed more favorably than players who put up amazing numbers on losing teams. This is fuelled by the notion that players on great teams often sacrifice personal accolades for the success of the team, which is warmly embraced by the media.  


<br/>

## Generalized Additive Models (GAM) <a name="gam"></a>
Previously, we used a logistic regression model to estimate a player’s probability of being named All-NBA at the end of a season. However, this GLM model assumes a linear relationship between the predictors and the log-odds of All-NBA selection, which may not be true. GAM models on the other hand do not assume a priori any specific structure for the relationship between the predictors and the outcome of All-NBA selection. In fact, GAMs can be used to model non-linear effects of these predictors on the response variable with the aid of spline functions. In a sense, we can consider GAM models as a more flexible version of GLM models. More information on GAMs can be found [here](https://christophm.github.io/interpretable-ml-book/extend-lm.html).


When fitting the GAM model, we left all hyper-parameters with their default values. A random search on the lambda penalty hyper-parameter was performed, however was found to have weaker performance when applied to the test-set.


### GAM 2020 Predictions

```{r, gampred, echo=FALSE}
gam_pred = read.csv("gam_predictions_2020.csv")

to_show = gam_pred[, -c(2,3,5)]

to_show$Probability = round(to_show$Probability,3)*100

knitr::kable(to_show, 
             caption = "Logistic Regression 2020 All-NBA predictions",
             digits = 2)
```



## K Nearest Neighbours (KNN) <a name="knn"></a>

Before applying the KNN algorithm, we standardized all continuous features in our data. Then, to choose an optimal value of k, we performed the algorithm with varying values of k and looked at the corresponding error rates. As shown in the Error Rate vs. K value graph below, k = 14 corresponded to the value with the lowest error rate. 

![](knn.png)

### KNN 2020 Predictions
```{r, knnpred, echo=FALSE}
knn_pred = read.csv("knn_predictions_2020.csv")

to_show = knn_pred[, -c(2,3,5)]

to_show$Probability = round(to_show$Probability,3)*100

knitr::kable(to_show, 
             caption = "KNN 2020 All-NBA predictions",
             digits = 2)
```



## Support Vector Machine (SVM) <a name="svm"></a>
<!--
For the SVM algoritm, a hyperplane that separates the binary outcomes is chosen such that the distance between the hyperplane and the support vectors (i.e. the data points that lie closest to the proposed hyperplane) is maximized. That is, the SVM algorithm finds an optimal separating hyperplane with the greatest possible margins.

The support vectors are the most influential points on the chosen hyperplane.
-->
A random search on the values of the parameters 'C' and 'gamma' was performed prior to fitting the SVM model, which were found to have similar performance to an SVM model equipped with the default settings of these parameters (C = 1, gamma = 1 / number of features).
A radial basis function was chosen for the kernel.


### SVM 2020 Predictions
```{r, svmpred, echo=FALSE}
knn_pred = read.csv("svm_predictions_2020.csv")

to_show = knn_pred[, -c(2,3,5)]

to_show$Probability = round(to_show$Probability,3)*100

knitr::kable(to_show, 
             caption = "SVM 2020 All-NBA predictions",
             digits = 2)
```



<br/>


## XGBoost <a name="xgb"></a>
<!--Need to tune hyperparameters
-->

![](xgb_feature_importance.png)

### XGBoost 2020 Predictions
```{r, xgbpred, echo=FALSE}
xgb_pred = read.csv("xgb_predictions_2020.csv")

to_show = xgb_pred[, -c(2,3,5)]

to_show$Probability = round(to_show$Probability,3)*100

knitr::kable(to_show, 
             caption = "XGBoost 2020 All-NBA predictions",
             digits = 2)
```

<br/>

## Appendix

All R and Python code to scrape data and fit the models can be found [here](https://github.com/petertea96/all_nba).

### Feature Variables <a name="features"></a>

- PTS: Points scored per 100 possessions
- AST: Assists per 100 possessions
- STL: Steals per 100 possessions
- BLK: Blocks per 100 possessions
- MP: Minutes played
- ORB: Offensive rebounds per 100 possessions
- DRB: Defensive rebounds per 100 possessions
- TOV: Turnovers per 100 possessions
- PF: Personal fouls per 100 possessions
- OFF Rating: Offensive rating (An estimate of points produced per 100 possessions)
- DEF Rating: Defensive rating (An estimate of points allowed per 100 possessions)
- Team Performance: A team's total # wins / League total # wins


### How does All-NBA voting work?
Selection is controlled by a panel of sportswriters and broadcasters (i.e. the media) who select players for the All-NBA 1st, 2nd and 3rd Teams by position. A tally of all the votes are then taken to determine the results of All-NBA selections for all three teams. Players placed on a 1st All-NBA team ballot are awarded 5 points, while players placed on a 2nd All-NBA team ballot are awarded 3 points and 1 point for a 3rd All-NBA team ballot.


<br/>

### Total Votes

The total voting results for all players receiving a vote can be found in the table below.


```{r, predictions, echo =FALSE}
dat1 <- read.csv(file = "votes_data.csv")

dat1 <- sapply(dat1, as.character)
dat1[is.na(dat1)] <- " "


colnames(dat1) <- c("Position", "Player", "Total Votes")

knitr::kable(dat1, 
             caption = "Model Results",
             digits = 2)
```

A more in depth breakdown of the votes for each model, can be found in the model's respective subsection.